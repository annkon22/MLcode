{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140c350c",
   "metadata": {},
   "source": [
    "## Regression model with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad19ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefeffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdb333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.getcwd(), 'forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56081cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e098541",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: not scaled or transformed with area as a target variable. \n",
    "# area is very skewed towards 0, it can have a bad impact on the result.\n",
    "#     features: ['FFMC','temp','RH','wind','rain', 'ISI']\n",
    "#     target: 'area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f6fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytoch dataset\n",
    "batch_size = 20\n",
    "\n",
    "def get_dl(inputs, targets, batch_size):\n",
    "    dataset = TensorDataset(torch.tensor(inputs, dtype = torch.float32), torch.tensor(targets, dtype = torch.float32))\n",
    "\n",
    "    # we'll split data into train and test using random split \n",
    "    train, val = random_split(dataset, [450, 67])\n",
    "\n",
    "    # create data loaders\n",
    "    train_dl = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    val_dl = DataLoader(val, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4aa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 data\n",
    "inputs1 = df[['FFMC','temp','RH','wind','rain', 'ISI']].values\n",
    "target1 = df['area'].values\n",
    "\n",
    "train_dl1, val_dl1 = get_dl(inputs1, target1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479f6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output sizes are the same for all models. input = 1 and putput = 6.\n",
    "output_size = 1\n",
    "input_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple linear model\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.linear(x)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7f9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_dl, epochs):\n",
    "    history = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch # %s' %e)\n",
    "        losses = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            print(f'    Batch # %s' %i)\n",
    "\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            loss.backward()\n",
    "            print('Loss: %s' %loss)\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        history.append(losses/len(train_dl))\n",
    "    return history\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6a9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, val_dl):\n",
    "    \n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_dl):\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            losses += loss\n",
    "    val_loss = losses/len(val_dl)\n",
    "    \n",
    "    return val_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2f066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(figsize = (10, 6))\n",
    "    plt.plot(history, '-x')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfebdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(32.8827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.7394, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.5716, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.6232, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.6983, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.1354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(19.8193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.0470, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(2.0681, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.4168, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.5641, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.2869, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.5277, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(16.6688, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(15.1381, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(20.1297, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(11.0764, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.2628, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.8568, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.4757, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(16.4640, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.6507, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.9256, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(18.9330, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.2360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.1429, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.6288, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.2808, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(12.9499, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.3234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.4979, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(13.0349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.7797, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(29.3144, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.9914, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.6053, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.9172, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(24.5703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.7691, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.3003, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.5715, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(13.7368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.8245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(15.2602, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.5768, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.5941, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(11.5443, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.4325, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.1059, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(15.7661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.5779, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.1099, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.9902, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(20.1972, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.0779, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.9408, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.3409, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(19.0054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(11.4661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.8266, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.2615, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.3018, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(19.7535, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(19.8153, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.6221, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.0688, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.8345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.7570, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.9160, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(22.7351, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(18.6272, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.7108, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.3050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(24.8057, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(21.8202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.4372, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.5318, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.4026, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.6195, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(24.4792, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.1885, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.6431, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.9142, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(9.3176, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(13.8418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(2.1373, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.4253, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.4934, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.5161, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.5465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.3391, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.8016, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(6.8920, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.3784, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.6989, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(15.5727, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.7940, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.6029, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(11.3000, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(12.2489, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.7121, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.5413, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(24.4047, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.0555, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.5541, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(12.6238, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(19.2112, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(10.6211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(11.9297, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.5974, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.2820, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.4033, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(12.6917, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.6622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(31.2437, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(17.8151, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(2.2229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.7844, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.1773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.5646, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.6284, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.5815, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.5835, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.5605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(19.5355, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.7696, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.9667, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(17.6240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.0423, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(10.5122, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(14.8574, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.7425, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.0220, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(12.3739, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(25.5884, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.6913, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.6615, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.3862, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(14.3638, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.8263, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.9597, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(17.3236, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(2.7944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.0787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.4365, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.0686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.3977, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(15.7860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.4357, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.5923, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(11.5091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.7228, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(10.1168, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.8258, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.1411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(1.5858, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(21.5962, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.7982, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(14.7072, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(17.7732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.7165, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(5.5579, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.1171, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(11.7656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.8704, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.7691, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.8075, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.1872, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.3263, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.2586, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.0936, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(16.9011, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.1734, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.2464, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.0682, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(31.3706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.7023, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.1138, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.4568, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(18.7500, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(19.0534, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(19.4768, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.9835, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(19.3924, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(6.5127, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.5206, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(19.9609, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.9783, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.7654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(14.8467, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(2.9104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(18.0762, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.9913, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.1019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(23.1681, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.2278, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.4848, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.0795, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9154, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(15.2001, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.1441, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(26.6571, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(20.9365, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.3231, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.5440, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.5561, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.4048, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(12.5349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.4418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(15.7797, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(25.7703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(16.5138, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.7009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.8245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.9326, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.2091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.3184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.6858, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(18.1792, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(14.9479, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.6381, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(16.9569, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.7155, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.5392, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.2906, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.2232, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.3575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(13.1951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.0996, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(10.3830, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(9.7651, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(20.3368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.5091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.9424, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.0435, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.9719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(13.2045, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.8581, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(16.4422, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(18.2540, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(18.0303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(8.1401, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.3450, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.1035, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.6632, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(15.2350, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.8337, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.8248, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.3405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.6241, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.2233, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.3207, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(17.6488, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(9.0061, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(18.8878, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.5003, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(17.1266, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.7821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(11.0282, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.3106, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.3276, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.0740, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(13.6081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(17.8168, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.3926, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.6349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.6824, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.5413, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.4986, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.7391, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(21.5261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(2.2172, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.2590, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(21.5821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.8054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(25.6289, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(11.3042, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(19.3595, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(21.6665, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.7133, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.0076, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(14.6184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.8476, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(25.4663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.3973, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.1214, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.8281, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.4849, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.9231, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.8769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.7983, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(2.1865, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.7665, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.4411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.1280, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.1013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(16.6179, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.4090, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(23.4862, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(7.5312, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.4250, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.1944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.9124, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.0868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.3331, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.5619, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.0284, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(11.2665, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.4705, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.7551, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(20.4641, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.1575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(15.5331, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.2218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.7527, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(15.2082, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.2665, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(26.7139, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.4271, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.9961, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.7357, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(12.6775, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(2.9308, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.2082, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.8324, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.8927, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.9702, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(25.0291, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.5949, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.7671, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.1240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(9.1347, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(11.1408, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(13.1572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.0498, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.0054, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(1.9097, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(15.4360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(14.3175, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(21.2827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(13.4877, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.5045, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.3329, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.7764, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(15.8143, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(3.4825, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(17.4716, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.1951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(17.3360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.9622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.3092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.1637, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.3653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.1736, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.4682, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(16.9833, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(16.2546, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.1623, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(15.5427, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9045, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(21.3832, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.1579, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.6491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.3700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.4955, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.9047, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.6382, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.9770, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(11.0958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.6443, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.9191, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.0815, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.9192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(22.6600, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.1506, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.8461, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.0264, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.3775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(12.2044, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(16.2767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(21.9946, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.4870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.6898, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(11.0167, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.7883, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.3161, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(17.9775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.3901, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.0340, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.9501, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(26.6628, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(15.9353, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(1.6897, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(19.9016, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.9136, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.6814, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.1437, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(13.3762, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.4547, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.0724, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.3801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(12.5348, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.7371, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.3149, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.1659, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(18.8770, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(22.9980, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.1369, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.1631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.6333, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(14.2196, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.0139, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(2.5919, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(18.0498, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(16.3645, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(11.0401, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(15.8870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(9.1424, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(8.1859, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(15.6471, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.3160, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.4943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.4549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.7576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.4644, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.4465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.7760, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.7675, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(9.0157, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.0581, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(13.9470, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(10.8816, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(20.4344, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.0755, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.4370, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(19.0386, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(31.0436, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(7.6639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.0368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(11.2219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.7637, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(8.8791, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.6639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(12.6239, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(19.1911, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.1818, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.5399, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.6963, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.8893, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(15.8628, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(15.2677, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(12.3657, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.6404, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.8047, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.0862, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(14.4127, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.5894, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.2146, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(21.1053, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.3103, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b311a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1dea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(42.2985, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(38.9096, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(35.1455, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(36.1710, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(33.6125, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(55.3090, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(36.4908, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(48.3216, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(41.5439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(31.9218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(29.1432, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(26.6368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(26.7833, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(36.9767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(28.8580, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(38.8959, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(28.7974, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(21.0140, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(20.1604, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(28.0574, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(33.4298, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(22.1664, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(18.2151, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(26.1231, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(26.6940, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(24.5269, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.1957, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.2318, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(18.0680, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(21.1390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(19.9440, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.9184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(15.8162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.5199, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.9090, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.9512, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.6429, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(2.6682, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.7236, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(22.5918, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.9076, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.3990, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.1555, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(10.8031, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.2407, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.2966, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(13.3173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.1943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.5361, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.3202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.5975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.2451, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.2600, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.5135, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(21.3530, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.7927, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(25.1320, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.5857, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.0970, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.1981, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(33.3944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.7589, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(14.5693, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.0173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.8033, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(2.9535, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(10.6901, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.5654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(14.9357, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(10.4901, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(14.5313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.4288, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.4695, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.4541, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.0094, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.3387, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.0787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.7921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.3424, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.7448, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.8481, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.9703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.8289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(15.0743, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.0223, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(23.3276, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(17.1459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.4700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(17.7461, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.5601, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.8955, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.7492, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(7.0944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.4418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.4659, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.4355, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(5.5929, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.5513, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(18.5789, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.9752, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.4278, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.2696, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(10.4555, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.8903, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(2.6771, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(13.9320, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(18.0005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.6947, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(20.5227, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.0504, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(13.6312, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(13.9673, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(14.3083, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.6008, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.7653, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(7.7540, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.3708, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(13.7435, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(35.1708, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.3314, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(26.8185, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.5219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.6154, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.5890, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.5702, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.5127, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(18.7737, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.3831, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.7585, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.0092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.4900, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.0644, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.4288, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(1.7698, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.8331, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.6818, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.6225, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(19.1904, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(8.8734, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(18.7530, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.4503, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.3552, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.7013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.8369, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.5559, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.3315, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.0760, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(18.6162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(9.8809, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(9.4269, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(18.3798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(17.7674, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.1956, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.7326, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.2411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.8370, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.5458, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.7812, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(2.6091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.3145, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.1314, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(6.3356, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.2428, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.0197, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.3860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.5248, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.9654, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.7705, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(13.0081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.7030, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(17.3456, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(21.5469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(14.1855, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.3751, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(15.2681, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(2.0115, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.0959, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.9711, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.7245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(13.5409, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.4741, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.6732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.7833, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.4293, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(7.6958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.2007, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.2261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(2.1405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.3618, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.8407, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(12.6383, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(19.1486, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(12.4008, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(14.3582, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(19.5897, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.1029, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.6525, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.8490, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(11.8889, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.4239, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.6142, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.3400, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.8038, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.9146, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(22.1787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(13.6035, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.2428, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(21.3466, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.7531, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(14.4860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.5208, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.9350, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.1812, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.1990, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(15.2833, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.8229, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(14.0397, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.3523, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(17.4625, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(12.6412, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.6162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.7301, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(20.7098, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.1173, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(10.7649, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(2.1914, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.4853, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(13.3953, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.9202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.3553, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(24.3734, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(20.8215, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.7103, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.6986, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(15.1251, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(2.5667, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.7781, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.2009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(11.8620, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.3934, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.5079, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.7549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(26.1110, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.8944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.8295, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.6491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.6933, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.1123, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.0610, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.1626, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.6583, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.9333, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.3365, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(7.4791, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(13.6302, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(11.7997, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(18.0967, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.8831, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.7928, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.7450, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.1405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.1528, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.7250, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.9857, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(11.5552, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.3852, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.3522, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(11.4566, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(2.6987, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(2.4521, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.6694, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.2325, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(13.2014, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(16.1635, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.1942, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(22.7148, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(22.0516, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.2951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.7703, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.6120, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.9871, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.4676, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.4549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.4802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.8917, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.5837, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.4095, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(16.5776, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(11.6928, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.0189, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(14.0706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(25.0616, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.7129, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.7370, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.7171, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(17.1407, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.5435, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.5489, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.3954, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(6.9335, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.6360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.7719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(18.0767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(11.7008, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.1542, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.0893, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(12.8278, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(10.3792, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(4.2679, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(9.3344, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.0984, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(11.0861, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.7079, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(11.1892, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(21.0471, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(26.7632, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.8433, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.5787, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.1342, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.4564, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.1083, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.6502, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(7.2286, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.3056, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(18.6282, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.5650, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(13.7051, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.7666, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.2730, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(2.4364, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.9160, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.5095, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.1783, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.1011, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(1.5289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.3948, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(26.0891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.4237, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.9905, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.4363, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.4542, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.4853, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(24.4301, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.1732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.0586, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(13.2746, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.9455, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.0886, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(16.9354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(11.3045, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(13.4808, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.9319, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.4290, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(13.6645, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(8.0773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.3234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(14.2434, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.7143, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.4234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.1499, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(12.0091, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.4753, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(11.0664, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.6714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(18.1059, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(2.4668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(12.3150, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(1.5035, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(5.1583, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.7291, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.1334, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.2069, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.0988, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.4113, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(17.6182, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.0193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(17.3627, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.4302, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(20.6563, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.8218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.3128, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(17.6027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.1815, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.6860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(12.5677, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.9324, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(14.8543, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.0969, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(14.8300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.3535, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(17.7152, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(4.9043, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.7864, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.5053, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.6146, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(14.2849, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.3334, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.2148, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.3001, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(11.1508, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.1456, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.8605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.4828, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(15.6798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(2.9576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.6159, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.1767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(17.9769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(15.1834, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.0267, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.3748, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.6841, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(18.6933, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(23.5468, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(3.9002, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.1256, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(11.2240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.7918, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.8311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.2607, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.3189, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(16.6153, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.1584, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(20.0910, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.5990, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.7892, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.6490, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.3538, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(9.5836, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.1070, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(23.3103, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(3.2718, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.4740, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.3821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.1525, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(3.7256, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(19.2944, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(4.1275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.7740, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.1132, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.1013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.9655, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.8452, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.6767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(17.4126, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.5446, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.6056, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(18.9785, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(12.9556, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(23.7749, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(3.9099, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.6169, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.2135, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(2.0209, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(22.6327, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.1615, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.1799, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.8058, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.2401, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.9160, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.0001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30cdbb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5123d4",
   "metadata": {},
   "source": [
    "This model uses all unscaled variables and a traget too skewed to 0. Training loss per epoch goes down but has many peaks. The model finds it hard to converge. The same demonstrates the evaluation scores. The smaller learning rate performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe9ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a070c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log(x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c67d53f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-35af0a812bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model 2 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We log transform are to get more homogenious values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area_log'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlog_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FFMC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RH'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ISI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-35af0a812bb2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model 2 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We log transform are to get more homogenious values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area_log'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlog_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FFMC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RH'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wind'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ISI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_transform' is not defined"
     ]
    }
   ],
   "source": [
    "# model 2 data\n",
    "# We log transform are to get more homogenious values\n",
    "df['area_log'] = df['area'].apply(lambda x: log_transform(x))\n",
    "\n",
    "inputs2 = df[['FFMC','temp','RH','wind','rain', 'ISI']].values\n",
    "target2 = df['area_log'].values\n",
    "\n",
    "train_dl2, val_dl2 = get_dl(inputs2, target2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad98af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr= 0.001)\n",
    "history2 = train_model(model2, optimizer, train_dl2, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2551c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluate on training: {:.4f}'.format(val_model(model1, train_dl2)))\n",
    "print('Evaluate on validation : {:.4f}'.format(val_model(model1, val_dl2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c575c9",
   "metadata": {},
   "source": [
    "Second model uses as a target area_log, that is a log transformed area. This may help to get better results. What we see is that the loss drops significantly and then stays on the same levels having peaks up an down.\n",
    "Transforming the target helped a model to converge faster though still at certain level it is hard for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20852f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555da56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8b635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
