{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140c350c",
   "metadata": {},
   "source": [
    "## Regression model with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad19ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefeffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdb333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.getcwd(), 'forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56081cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e098541",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: not scaled or transformed with area as a target variable. \n",
    "# area is very skewed towards 0, it can have a bad impact on the result.\n",
    "#     features: ['FFMC','temp','RH','wind','rain', 'ISI']\n",
    "#     target: 'area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f6fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytoch dataset\n",
    "batch_size = 20\n",
    "\n",
    "def get_dl(inputs, targets, batch_size):\n",
    "    dataset = TensorDataset(torch.tensor(inputs, dtype = torch.float32), torch.tensor(targets, dtype = torch.float32))\n",
    "\n",
    "    # we'll split data into train and test using random split \n",
    "    train, val = random_split(dataset, [450, 67])\n",
    "\n",
    "    # create data loaders\n",
    "    train_dl = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    val_dl = DataLoader(val, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4aa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 data\n",
    "inputs1 = df[['FFMC','temp','RH','wind','rain', 'ISI']].values\n",
    "target1 = df['area'].values\n",
    "\n",
    "train_dl1, val_dl1 = get_dl(inputs1, target1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479f6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output sizes are the same for all models. input = 1 and putput = 6.\n",
    "output_size = 1\n",
    "input_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple linear model\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.linear(x)\n",
    "        return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7f9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_dl, epochs):\n",
    "    history = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch # %s' %e)\n",
    "        losses = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            print(f'    Batch # %s' %i)\n",
    "\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            loss.backward()\n",
    "            print('Loss: %s' %loss)\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        history.append(losses/len(train_dl))\n",
    "    return history\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6a9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, val_dl):\n",
    "    \n",
    "    losses = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(val_dl):\n",
    "            out = model(inputs).squeeze() #predictions\n",
    "            loss = F.l1_loss(out, targets) # loss\n",
    "            losses += loss\n",
    "    val_loss = losses/len(val_dl)\n",
    "    \n",
    "    return val_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2f066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(figsize = (10, 6))\n",
    "    plt.plot(history, '-x')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfebdaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(28.9138, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(25.9596, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(13.0709, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(14.8051, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.1308, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.1694, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(18.2970, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(41.2117, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(18.8612, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(19.8644, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(5.7240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.9926, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.9952, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.3966, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.8089, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(62.1482, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(18.5459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(11.1418, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(20.8469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.9018, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(12.3719, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.1972, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.7237, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(6.7305, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(64.3717, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(2.7094, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(23.3289, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(17.7909, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.4659, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.6377, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(12.2922, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(14.0789, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(52.0059, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.4541, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(24.2560, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.3469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.9019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.5469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.9603, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(9.2373, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(7.5237, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.8673, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.4338, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.9968, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.0379, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(22.2091, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(27.7392, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(3.6275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.8073, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(26.2461, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(70.0075, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(4.1165, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.3760, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.2260, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(11.6613, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.3048, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.7100, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.4148, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.1858, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.5081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.8567, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(19.1309, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(31.3291, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.7313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.8179, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.8653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(48.3023, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(10.4749, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.1093, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(10.7295, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(26.0137, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(2.5798, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.1781, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(44.3830, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.0868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(12.0217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.5864, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.6012, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(23.1559, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.9983, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.4373, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(9.9465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(23.2152, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(12.1150, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.1933, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(19.0576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.6209, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.1538, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(63.3044, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.8120, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(17.7398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.1212, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(6.1513, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.2925, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(19.1638, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.8449, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(69.8451, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(3.5663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.0378, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.3319, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.4446, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.8816, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(22.8930, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.2491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.7405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.4346, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(14.0549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(19.1675, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(49.3881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.2725, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.0487, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.7311, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(11.0739, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.6906, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.9287, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(12.4009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(14.4157, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(12.9447, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.3503, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.4883, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.1041, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.3042, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.4474, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.2211, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(61.1486, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(9.5429, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(5.7997, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(17.0728, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(42.4923, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(8.4366, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.7595, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(4.7844, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.0093, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(36.8686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.5066, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(19.1649, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.8078, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(18.5092, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(13.9202, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.9271, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(57.6491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.8087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(11.3699, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.3763, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.9717, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.0684, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.2891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.9592, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(21.9335, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.0589, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(16.2943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.9670, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(12.6179, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.2368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(46.8111, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.4428, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(11.5649, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.3545, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(30.4188, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.2679, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.1020, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(11.2085, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.5310, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.9771, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(7.5723, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.7422, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.8344, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.0690, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.8733, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(96.0448, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(13.0975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(11.5519, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.3137, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(30.5922, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(11.9274, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.8764, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(18.8477, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(11.9565, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(22.1019, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.3816, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.5765, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.1355, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.2996, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(6.1833, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(16.8079, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.1611, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(8.9413, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(50.9489, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(7.2955, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.2432, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.0762, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(13.6333, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(19.8895, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(12.1619, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(59.8689, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.1940, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.6050, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(9.3488, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.6907, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.6434, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(21.9467, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.1207, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(34.2706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.9970, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(12.1846, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.1354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.6285, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(5.0066, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.0493, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.2593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.0492, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(19.3088, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(15.3002, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(10.3234, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.2104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.3238, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.4283, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.6341, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.3115, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.2491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(22.2877, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(14.1700, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(65.6999, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.8306, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(48.6697, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.0110, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(9.0240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(14.8931, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.9140, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(38.6241, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(10.4656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.8297, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(12.5809, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.3447, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.0025, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.4405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(42.1038, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(24.8587, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(18.4725, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.9453, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.6596, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(11.2085, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.8819, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.8222, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(13.4162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(59.1668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(16.6948, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(31.8868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.2551, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.1626, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(7.9658, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.7038, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.8055, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(3.4426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.2390, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(13.9553, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.4354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(13.0854, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.5169, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(51.1294, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(30.5354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(18.1890, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.8384, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(73.0663, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(13.3464, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(6.6227, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.9769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(9.0965, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(10.4340, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(5.6281, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.5469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(25.8586, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.6763, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.5742, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.2958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(18.1148, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(22.0936, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(46.3624, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.1856, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.2148, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(60.9679, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(20.1692, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.9328, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.8323, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.5183, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(20.2668, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(11.4300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.9826, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.1801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(16.4182, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(18.0118, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(11.0887, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(15.3912, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(11.0803, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.2448, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(4.8781, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.4817, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(15.0717, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(10.8429, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(8.1541, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(7.3074, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.0950, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(11.8135, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.8223, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(24.6145, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.8522, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.2170, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.1171, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(7.9750, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.0464, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(16.4658, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(15.2689, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(20.2269, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9016, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.7009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(13.5620, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(4.7615, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.2219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(100.1921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(11.3261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(27.2573, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.8111, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(4.0999, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(8.9142, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(27.5389, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.0046, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(22.4002, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.9217, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.9187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(14.5469, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(17.4777, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(54.6139, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(14.3710, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.3349, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.3623, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.2191, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(16.8714, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.5226, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(58.1502, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(12.6374, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(9.7599, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(12.5537, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.5585, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.9548, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.0054, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(9.0666, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(14.7931, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.0368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(9.9138, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(12.0774, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.1134, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(6.4588, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.3318, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(18.7007, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.3873, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.7218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.7802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.1067, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(13.6673, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.0677, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(28.6958, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(49.3682, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.9387, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.6856, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(18.5035, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(57.1303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(18.8485, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.4015, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(41.2624, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.4110, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.8134, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.7201, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(32.0392, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(11.5035, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(62.2822, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(6.5525, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(9.7915, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(18.3763, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(15.3044, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.9163, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(10.5318, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.1162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(22.0206, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(18.3283, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.2802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(17.7848, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.2134, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.2440, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.7637, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.6902, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.7896, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(20.7219, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(64.1114, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(9.9621, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.6057, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(20.8802, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(14.6971, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(9.5488, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(13.1832, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(17.2644, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(51.5855, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(18.5210, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.0010, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.0497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.1272, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.9033, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.8517, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.7454, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(10.5490, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(8.2340, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(8.9617, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(6.6715, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.3044, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(12.0165, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(24.7672, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(11.8538, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.1213, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(18.5479, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(11.4862, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.0684, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.1335, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.8549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.5655, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(33.5483, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.1746, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.4167, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(60.6156, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.2467, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.8502, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(10.2399, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(45.0089, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(22.4546, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.9605, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.0734, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(17.3346, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.4945, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.5020, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(17.8359, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(69.7353, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(19.1005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(10.1979, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.0836, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(19.6268, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(4.0659, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(11.4398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.4413, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.2593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(43.2462, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(2.9237, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(18.6074, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(8.1643, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.9546, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.9037, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(12.2033, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(21.1092, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.7251, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(26.0543, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(13.1058, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.8809, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.6698, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b311a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b1dea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "    Batch # 0\n",
      "Loss: tensor(49.6294, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(44.9866, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(49.3343, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(50.2099, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(46.5407, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(51.1187, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(45.1500, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(48.3485, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(39.7642, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(43.2841, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(76.9706, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(96.5884, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(40.6361, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(42.6184, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(38.8292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(36.1314, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(38.2166, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(35.8296, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(35.3157, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(44.4116, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(36.9177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(35.4572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(43.2541, grad_fn=<L1LossBackward>)\n",
      "Epoch # 1\n",
      "    Batch # 0\n",
      "Loss: tensor(28.9684, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(73.7310, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(29.2699, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(30.1286, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(27.7918, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(33.4261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(41.7954, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(22.0810, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(31.7240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(24.4830, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(24.0008, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(21.2345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(71.2491, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(23.1317, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(20.3435, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(19.7425, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(24.7661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(16.4997, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(26.5181, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(15.7550, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(22.9203, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(16.6118, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(9.6321, grad_fn=<L1LossBackward>)\n",
      "Epoch # 2\n",
      "    Batch # 0\n",
      "Loss: tensor(75.4539, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(16.4744, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(13.1428, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(14.4914, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(14.0151, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(10.3013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(17.8267, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(27.4991, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(15.1911, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(16.5098, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(11.7779, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(7.5431, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(44.3306, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(26.1865, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.7214, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.0773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.8591, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.2115, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(17.4620, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.1720, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(14.5525, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(6.0080, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(5.3012, grad_fn=<L1LossBackward>)\n",
      "Epoch # 3\n",
      "    Batch # 0\n",
      "Loss: tensor(15.4015, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.4686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.2536, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.4527, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(2.3346, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.1068, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(70.5530, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.1414, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(45.9391, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(16.0384, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(10.6677, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(8.4097, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.2439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(31.3462, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(10.9899, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(3.9763, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.0359, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(9.3003, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.8204, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.6062, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.0081, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.3944, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(15.5910, grad_fn=<L1LossBackward>)\n",
      "Epoch # 4\n",
      "    Batch # 0\n",
      "Loss: tensor(12.9666, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(16.7833, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.1303, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(63.7144, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.2881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.4005, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(7.2177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.5151, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(11.0939, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(3.4253, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(4.8017, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(21.3247, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.1870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(17.1598, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.6290, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(57.3934, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(12.1594, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(6.1466, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(20.7108, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.9432, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(9.3694, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(22.9698, grad_fn=<L1LossBackward>)\n",
      "Epoch # 5\n",
      "    Batch # 0\n",
      "Loss: tensor(8.6113, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(12.4874, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(3.9382, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.6761, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(6.5496, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(8.5190, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.8524, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.2764, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(14.5868, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(15.0104, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(21.8153, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(73.0559, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.4821, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(16.2569, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.4068, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.8954, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.7240, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(10.3607, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(20.0302, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(39.5891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.6810, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(14.6075, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(7.9708, grad_fn=<L1LossBackward>)\n",
      "Epoch # 6\n",
      "    Batch # 0\n",
      "Loss: tensor(2.7650, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(5.9027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(17.3639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(72.3866, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.8208, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(13.8902, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.0845, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(14.1355, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(16.9879, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(5.1879, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(14.1773, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.1404, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.8439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.2576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(14.5628, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.2948, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(7.0683, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(14.1339, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(14.8726, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.0739, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(11.1360, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(8.2046, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(79.5912, grad_fn=<L1LossBackward>)\n",
      "Epoch # 7\n",
      "    Batch # 0\n",
      "Loss: tensor(17.7859, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(10.7405, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(10.4752, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(5.4056, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.3145, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.5423, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(13.7932, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(5.9615, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(3.9251, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(14.2631, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(60.3765, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(3.9680, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(16.0472, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.7642, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(14.6032, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.0801, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(15.3843, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(12.4277, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(47.3373, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(3.4336, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(5.8990, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.8082, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(10.7408, grad_fn=<L1LossBackward>)\n",
      "Epoch # 8\n",
      "    Batch # 0\n",
      "Loss: tensor(5.9975, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.3562, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.5795, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.6810, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(24.9661, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.6778, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.0494, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.1023, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.8992, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(44.0985, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(7.2457, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(16.4445, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(2.7261, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.1817, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(63.0411, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.4908, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(17.5162, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(20.2959, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(10.5655, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.2887, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.1063, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.2292, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(22.6662, grad_fn=<L1LossBackward>)\n",
      "Epoch # 9\n",
      "    Batch # 0\n",
      "Loss: tensor(16.3110, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(9.4325, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.2007, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(3.9177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(54.3443, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(17.4387, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.1118, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.8711, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.0977, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(67.0590, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(14.5082, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(2.0653, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(11.5450, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(26.0481, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.4568, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.0056, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(11.2326, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(5.1427, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.2172, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.0838, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(3.1827, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.3701, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(18.2449, grad_fn=<L1LossBackward>)\n",
      "Epoch # 10\n",
      "    Batch # 0\n",
      "Loss: tensor(6.2520, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(16.7434, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(19.2995, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(58.3593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.6759, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(5.5759, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.0720, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(15.4560, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.6721, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(10.5877, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(15.9567, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(13.2612, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(7.6553, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(6.1656, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(7.0107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(44.0900, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(13.3686, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.0702, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.8078, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(17.3143, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.6594, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(11.2271, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(3.0992, grad_fn=<L1LossBackward>)\n",
      "Epoch # 11\n",
      "    Batch # 0\n",
      "Loss: tensor(5.0008, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(65.6212, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.7650, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.4685, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(21.6547, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(11.4805, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(3.4742, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(8.8428, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.9488, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(6.7536, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(13.0241, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(10.1359, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(4.0780, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(4.5233, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(3.4500, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(12.8865, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.7157, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(13.0916, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(49.5646, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(16.3399, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(16.9039, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(17.3774, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.9654, grad_fn=<L1LossBackward>)\n",
      "Epoch # 12\n",
      "    Batch # 0\n",
      "Loss: tensor(4.0245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(6.3192, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(6.5439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(2.6575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(63.6482, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(9.8608, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(32.4398, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(10.3846, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(4.4891, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(16.4572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(16.4119, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(42.8459, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.9691, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.3614, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(6.7350, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(8.1667, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(29.3465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.7732, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.4591, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(11.6572, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(11.5074, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.0301, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(4.5896, grad_fn=<L1LossBackward>)\n",
      "Epoch # 13\n",
      "    Batch # 0\n",
      "Loss: tensor(7.3027, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(69.9843, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(18.7927, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(4.1497, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(4.9575, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.5524, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(16.0690, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(3.8402, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(8.6009, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(56.2426, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(8.8302, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.1954, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.2882, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.0716, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.6877, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(6.8347, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(8.7786, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(6.9580, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(21.0188, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(5.9097, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(8.4356, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(17.3966, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(31.4675, grad_fn=<L1LossBackward>)\n",
      "Epoch # 14\n",
      "    Batch # 0\n",
      "Loss: tensor(20.4534, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(13.3867, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(12.2221, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(8.0789, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.6028, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(6.6430, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(12.7639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(4.4851, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(2.8096, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(1.9191, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(17.8105, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(11.8030, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(8.9049, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(47.3873, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(23.1022, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(5.7041, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(13.8218, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(57.6915, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.2553, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(14.4265, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.7417, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(5.7465, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(5.2859, grad_fn=<L1LossBackward>)\n",
      "Epoch # 15\n",
      "    Batch # 0\n",
      "Loss: tensor(6.2087, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(4.3635, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(28.3280, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(13.6454, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(44.4396, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(13.9151, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(7.0711, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(19.0111, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(6.8635, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(58.5917, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(4.7886, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(10.2354, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.4020, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(7.8593, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(9.3283, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(11.4474, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.5639, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(15.4544, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(17.9102, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(12.1738, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.7312, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(4.3478, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(8.9167, grad_fn=<L1LossBackward>)\n",
      "Epoch # 16\n",
      "    Batch # 0\n",
      "Loss: tensor(10.5480, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(22.4803, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(43.3543, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.9113, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(15.5848, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(15.9145, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.7646, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(14.2112, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(7.5152, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.8921, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(61.1765, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(10.3741, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(5.2193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(10.3020, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.8313, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(9.8509, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(3.9556, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(2.2633, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(5.7107, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(13.5860, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(17.7548, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(7.1070, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(39.8946, grad_fn=<L1LossBackward>)\n",
      "Epoch # 17\n",
      "    Batch # 0\n",
      "Loss: tensor(13.9345, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(16.2439, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(4.8177, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(6.4799, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(3.0840, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(7.4314, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(8.0042, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(17.9460, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(21.5186, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(3.9549, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.9065, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(62.9997, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(3.8717, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(5.4726, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(5.0970, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(15.9870, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(10.5168, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(11.6452, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(3.3146, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(6.4375, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(56.3193, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(25.3293, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(11.4316, grad_fn=<L1LossBackward>)\n",
      "Epoch # 18\n",
      "    Batch # 0\n",
      "Loss: tensor(14.6669, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(23.2709, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(7.7444, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(2.4881, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(9.6175, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(52.1576, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(5.6968, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(9.6951, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.6866, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(18.0212, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(6.5047, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(6.7767, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(13.9775, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(18.2275, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(13.6789, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(7.6300, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(61.9205, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(11.0677, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(7.2612, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(7.2106, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(4.1909, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(1.5368, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(19.7924, grad_fn=<L1LossBackward>)\n",
      "Epoch # 19\n",
      "    Batch # 0\n",
      "Loss: tensor(18.8539, grad_fn=<L1LossBackward>)\n",
      "    Batch # 1\n",
      "Loss: tensor(65.5869, grad_fn=<L1LossBackward>)\n",
      "    Batch # 2\n",
      "Loss: tensor(5.0633, grad_fn=<L1LossBackward>)\n",
      "    Batch # 3\n",
      "Loss: tensor(12.7710, grad_fn=<L1LossBackward>)\n",
      "    Batch # 4\n",
      "Loss: tensor(10.8600, grad_fn=<L1LossBackward>)\n",
      "    Batch # 5\n",
      "Loss: tensor(24.5425, grad_fn=<L1LossBackward>)\n",
      "    Batch # 6\n",
      "Loss: tensor(40.7262, grad_fn=<L1LossBackward>)\n",
      "    Batch # 7\n",
      "Loss: tensor(15.3685, grad_fn=<L1LossBackward>)\n",
      "    Batch # 8\n",
      "Loss: tensor(5.0943, grad_fn=<L1LossBackward>)\n",
      "    Batch # 9\n",
      "Loss: tensor(2.6655, grad_fn=<L1LossBackward>)\n",
      "    Batch # 10\n",
      "Loss: tensor(2.8419, grad_fn=<L1LossBackward>)\n",
      "    Batch # 11\n",
      "Loss: tensor(10.8449, grad_fn=<L1LossBackward>)\n",
      "    Batch # 12\n",
      "Loss: tensor(19.3622, grad_fn=<L1LossBackward>)\n",
      "    Batch # 13\n",
      "Loss: tensor(21.0632, grad_fn=<L1LossBackward>)\n",
      "    Batch # 14\n",
      "Loss: tensor(4.8564, grad_fn=<L1LossBackward>)\n",
      "    Batch # 15\n",
      "Loss: tensor(4.0235, grad_fn=<L1LossBackward>)\n",
      "    Batch # 16\n",
      "Loss: tensor(6.4245, grad_fn=<L1LossBackward>)\n",
      "    Batch # 17\n",
      "Loss: tensor(8.1769, grad_fn=<L1LossBackward>)\n",
      "    Batch # 18\n",
      "Loss: tensor(4.9144, grad_fn=<L1LossBackward>)\n",
      "    Batch # 19\n",
      "Loss: tensor(10.3412, grad_fn=<L1LossBackward>)\n",
      "    Batch # 20\n",
      "Loss: tensor(9.8013, grad_fn=<L1LossBackward>)\n",
      "    Batch # 21\n",
      "Loss: tensor(13.7468, grad_fn=<L1LossBackward>)\n",
      "    Batch # 22\n",
      "Loss: tensor(2.0829, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = Linear()\n",
    "epochs = 20\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr= 0.0001)\n",
    "history1 = train_model(model1, optimizer, train_dl1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30cdbb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5123d4",
   "metadata": {},
   "source": [
    "This model uses all unscaled variables and a traget too skewed to 0. Training loss per epoch goes down but has many peaks. The model finds it hard to converge. The same demonstrates the evaluation scores. The smaller learning rate performs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
